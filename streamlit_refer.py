import streamlit as st
import tiktoken
import subprocess
# import pkg_resources
from loguru import logger

import os
import tempfile


from langchain.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import UnstructuredPowerPointLoader
from langchain_community.document_loaders import UnstructuredExcelLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS

# from streamlit_chat import message
from langchain_community.callbacks import get_openai_callback
from langchain.memory import StreamlitChatMessageHistory


# # Function to install packages
# def install_packages(*packages):
#     for package in packages:
#         try:
#             subprocess.check_call(['pip', 'install', package])
#             print(f"{package} has been successfully installed.")
#         except subprocess.CalledProcessError:
#             print(f"Failed to install {package}.")

# # List of packages to install
# packages_to_install = ['pandas', 'openpyxl']

# # Install packages
# install_packages(*packages_to_install)



# st.title('Shell Command Executor')

# # User input for the shell command
# command = st.text_input('Enter a shell command', value='echo Hello Streamlit!')

# # Button to execute the command
# if st.button('Execute'):
#     try:
#         # Execute the shell command
#         result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        
#         # Display the output
#         st.success('Command executed successfully!')
#         st.subheader('Output:')
#         st.text(result.stdout)
        
#         # Display any errors
#         if result.stderr:
#             st.error('Error:')
#             st.text(result.stderr)
#     except Exception as e:
#         st.error(f'An error occurred: {e}')


def main():
    st.set_page_config(
    page_title="FIDO2ChatBOT",
    page_icon=":books:")

    st.title("_FIDO2 :red[QA ChatBot]_ ğŸ”‘")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    with st.sidebar:
        model_selection = st.selectbox(
            "Choose the language model",
            ("gpt-3.5-turbo", "gpt-4-turbo-preview"),
            key="model_selection"
        )
        
        uploaded_files =  st.file_uploader("Upload your file",type=['pdf','docx','pptx','xlsx'],accept_multiple_files=True)
        openai_api_key = st.text_input("OpenAI API Key", key="Streamlit2", type="password")
        process = st.button("Process")

        st.title('Shell Command Executor')
        command = st.text_input('Enter a shell command', value='echo Hello Streamlit!')
        execute = st.button('Execute')
    if execute:
        try:
            # Execute the shell command
            result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        
            # Display the output
            st.success('Command executed successfully!')
            st.subheader('Output:')
            st.text(result.stdout)
        
            # Display any errors
            if result.stderr:
                st.error('Error:')
                st.text(result.stderr)
        except Exception as e:
            st.error(f'An error occurred: {e}')

    if process:
        if not openai_api_key:
            st.info("Please add your OpenAI API key to continue.")
            st.stop()
        files_text = get_text(uploaded_files)
        text_chunks = get_text_chunks(files_text)
        vetorestore = get_vectorstore(text_chunks)
     
        st.session_state.conversation = get_conversation_chain(vetorestore, openai_api_key, st.session_state.model_selection)

        st.session_state.processComplete = True

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant", 
                                        "content": "ì•ˆë…•í•˜ì„¸ìš”! FIDO ì¸ì¦ ì‹œìŠ¤í…œì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì„ ë¬¼ì–´ë´ì£¼ì„¸ìš”!"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    history = StreamlitChatMessageHistory(key="chat_messages")

    # Chat logic
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            chain = st.session_state.conversation

            with st.spinner("Thinking..."):
                result = chain({"question": query})
                with get_openai_callback() as cb:
                    st.session_state.chat_history = result['chat_history']
                response = result['answer']
                source_documents = result['source_documents']

                st.markdown(response)
                with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    for doc in source_documents:
                        st.markdown(doc.metadata['source'], help=doc.page_content)
                    


# Add assistant message to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def load_document(doc):
    """
    ì—…ë¡œë“œëœ ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , í•´ë‹¹ í¬ë§·ì— ë§ëŠ” ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

    ì§€ì›ë˜ëŠ” íŒŒì¼ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë¬¸ì„œ ë¡œë”(PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader)ë¥¼ ì‚¬ìš©í•˜ì—¬
    ë¬¸ì„œ ë‚´ìš©ì„ ë¡œë“œí•˜ê³  ë¶„í• í•©ë‹ˆë‹¤. ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ìœ í˜•ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
    - doc (UploadedFile): Streamlitì„ í†µí•´ ì—…ë¡œë“œëœ íŒŒì¼ ê°ì²´ì…ë‹ˆë‹¤.

    Returns:
    - List[Document]: ë¡œë“œ ë° ë¶„í• ëœ ë¬¸ì„œ ê°ì²´ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ìœ í˜•ì˜ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
    temp_dir = tempfile.gettempdir()
    file_path = os.path.join(temp_dir, doc.name)

    # íŒŒì¼ ì“°ê¸°
    with open(file_path, "wb") as file:
        file.write(doc.getbuffer())  # íŒŒì¼ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ì“´ë‹¤

    # íŒŒì¼ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    try:
        if file_path.endswith('.pdf'):
            loaded_docs = PyPDFLoader(file_path).load_and_split()
        elif file_path.endswith('.docx'):
            loaded_docs = Docx2txtLoader(file_path).load_and_split()
        elif file_path.endswith('.pptx'):
            loaded_docs = UnstructuredPowerPointLoader(file_path).load_and_split()
        elif file_path.endswith('.xlsx'):
            loaded_docs = UnstructuredExcelLoader(file_path).load_and_split()
        else:
            loaded_docs = []  # ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ìœ í˜•
    finally:
        os.remove(file_path)  # ì‘ì—… ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ

    return loaded_docs

def get_text(docs):
    doc_list = []
    for doc in docs:
        doc_list.extend(load_document(doc))
    return doc_list

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text)
    return chunks


def get_vectorstore(text_chunks):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” Hugging Faceì˜ 'jhgan/ko-sroberta-multitask' ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° í…ìŠ¤íŠ¸ ì²­í¬ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•˜ê³ ,
    ì´ ì„ë² ë”©ë“¤ì„ FAISS ì¸ë±ìŠ¤ì— ì €ì¥í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì €ì¥ì†ŒëŠ” í…ìŠ¤íŠ¸ ì²­í¬ë“¤ ê°„ì˜
    ìœ ì‚¬ë„ ê²€ìƒ‰ ë“±ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Parameters:
    - text_chunks (List[str]): ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸ ì²­í¬ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

    Returns:
    - vectordb (FAISS): ìƒì„±ëœ ì„ë² ë”©ë“¤ì„ ì €ì¥í•˜ê³  ìˆëŠ” FAISS ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤.

    ëª¨ë¸ ì„¤ëª…:
    'jhgan/ko-sroberta-multitask'ëŠ” ë¬¸ì¥ê³¼ ë¬¸ë‹¨ì„ 768ì°¨ì›ì˜ ë°€ì§‘ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” sentence-transformers ëª¨ë¸ì…ë‹ˆë‹¤.
    í´ëŸ¬ìŠ¤í„°ë§ì´ë‚˜ ì˜ë¯¸ ê²€ìƒ‰ ê°™ì€ ì‘ì—…ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. KorSTS, KorNLI í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ ë©€í‹° íƒœìŠ¤í¬ í•™ìŠµì„ ì§„í–‰í•œ í›„,
    KorSTS í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€í•œ ê²°ê³¼, Cosine Pearson ì ìˆ˜ëŠ” 84.77, Cosine Spearman ì ìˆ˜ëŠ” 85.60 ë“±ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
"""
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb

def get_conversation_chain(vetorestore, openai_api_key, model_selection):
    """
    ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ë²¡í„° ì €ì¥ì†Œ, OpenAI API í‚¤, ëª¨ë¸ ì„ íƒì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ ì²´ì¸ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

    Parameters:
    - vetorestore: ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì„œ ë˜ëŠ” ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    - openai_api_key (str): OpenAI APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ API í‚¤ì…ë‹ˆë‹¤.
    - model_selection (str): ëŒ€í™” ìƒì„±ì— ì‚¬ìš©ë  ì–¸ì–´ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤. ì˜ˆ: 'gpt-3.5-turbo', 'gpt-4-turbo-preview'.

    Returns:
    - ConversationalRetrievalChain: ì´ˆê¸°í™”ëœ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ì…ë‹ˆë‹¤.

    í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. ChatOpenAI í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ íƒëœ ëª¨ë¸ì— ëŒ€í•œ ì–¸ì–´ ëª¨ë¸(LLM) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    2. ConversationalRetrievalChain.from_llm ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ,
       - ê²€ìƒ‰(retrieval) ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë  ë²¡í„° ì €ì¥ì†Œì™€ ê²€ìƒ‰ ë°©ì‹
       - ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•  ë©”ëª¨ë¦¬ ì»´í¬ë„ŒíŠ¸
       - ëŒ€í™” ì´ë ¥ì—ì„œ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë°©ë²•
       - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜í™˜í• ì§€ ì—¬ë¶€ ë“±ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    3. ìƒì„±ëœ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_selection, temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vetorestore.as_retriever(search_type='mmr', verbose=True),
        memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True
    )

    return conversation_chain



if __name__ == '__main__':
    main()